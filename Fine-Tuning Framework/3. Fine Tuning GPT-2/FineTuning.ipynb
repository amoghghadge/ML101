{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install transformers -U\n",
    "# !pip install accelerate                   library that greatly speeds up training, allows us to automatically make use of multiple GPUs if available\n",
    "# !pip install trl                          library that lets us instantiate our trainer and define our training arguments, highly optimized for training transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/ml/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset   # api that hugging face provides where using the appropriate string that identifies the name of a dataset, you can\n",
    "# pass that into the load dataset function and it will download the dataset you desire\n",
    "DATASET_NAME = \"mlabonne/guanaco-llama2-1k\"        # dataset of size 1000, allows us to fine tune gpt 2 into a model that can actually perform question and answering\n",
    "\n",
    "dataset = load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)      # this dataset only has \"train\" key, because this dataset is primarily for training, other datasets may have \"test\" key for testing\n",
    "# the train key maps to dataset object which itself is a dictionary with the features being text (no lables just blocks of text) and it has 1000 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "training_dataset = dataset['train']\n",
    "print(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<s>[INST] Me gradué hace poco de la carrera de medicina ¿Me podrías aconsejar para conseguir rápidamente un puesto de trabajo? [/INST] Esto vale tanto para médicos como para cualquier otra profesión tras finalizar los estudios aniversarios y mi consejo sería preguntar a cuántas personas haya conocido mejor. En este caso, mi primera opción sería hablar con otros profesionales médicos, echar currículos en hospitales y cualquier centro de salud. En paralelo, trabajaría por mejorar mi marca personal como médico mediante un blog o formas digitales de comunicación como los vídeos. Y, para mejorar las posibilidades de encontrar trabajo, también participaría en congresos y encuentros para conseguir más contactos. Y, además de todo lo anterior, seguiría estudiando para presentarme a las oposiciones y ejercer la medicina en el sector público de mi país. </s>'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example question / answer pair in spanish (model will learn to chat in multiple languages)\n",
    "training_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<s>[INST] write me a 1000 words essay about deez nuts. [/INST] The Deez Nuts meme first gained popularity in 2015 on the social media platform Vine. The video featured a young man named Rodney Bullard, who recorded himself asking people if they had heard of a particular rapper. When they responded that they had not, he would respond with the phrase \"Deez Nuts\" and film their reactions. The video quickly went viral, and the phrase became a popular meme. \\n\\nSince then, Deez Nuts has been used in a variety of contexts to interrupt conversations, derail discussions, or simply add humor to a situation. It has been used in internet memes, in popular music, and even in politics. In the 2016 US presidential election, a 15-year-old boy named Brady Olson registered as an independent candidate under the name Deez Nuts. He gained some traction in the polls and even made appearances on national news programs.\\n\\nThe Deez Nuts meme has had a significant impact on popular culture. It has become a recognizable catchphrase that people use to add humor to everyday conversations. The meme has also been used to satirize politics and other serious issues. For example, in 2016, a group of activists in the UK used the phrase \"Deez Nuts for President\" as part of a campaign to encourage young people to vote in the EU referendum. </s><s>[INST] Rewrite the essay in a more casual way. Instead of sounding proffesional, sound like a college student who is forced to write the essay but refuses to do so in the propper way. Use casual words and slang when possible. [/INST] Yo, so you want me to write a 1000-word essay about Deez Nuts? Alright, fine. So, this whole thing started on Vine back in 2015. Some dude named Rodney Bullard made a video where he would ask people if they knew a rapper, and when they said no, he would hit them with the classic line: \"Deez Nuts!\" People loved it, and it became a viral meme.\\n\\nNowadays, Deez Nuts is used for all kinds of stuff. You can throw it out there to interrupt someone or just to be funny. It\\'s all over the internet, in music, and even in politics. In fact, during the 2016 US presidential election, a kid named Brady Olson registered as an independent candidate under the name Deez Nuts. He actually got some attention from the media and made appearances on TV and everything.\\n\\nThe impact of Deez Nuts on our culture is pretty huge. It\\'s become a thing that everyone knows and uses to add some humor to their everyday conversations. Plus, people have used it to make fun of politics and serious issues too. Like, in the UK, some groups of activists used the phrase \"Deez Nuts for President\" to encourage young people to vote in the EU referendum.\\n\\nThere you have it, a thousand words about Deez Nuts in a more casual tone. Can I go back to playing video games now? </s>'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another question / answer pair, model learns from prompt and response and picks up the pattern / learn to chat back and forth with us\n",
    "# we don't need labels, training examples are embedded within the block of text to help the model learn to predict the next token\n",
    "training_dataset[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<s>[INST] Ich möchte für ein Handy-Game den Flug von Papierfliegern simulieren. Was für Formeln können mir helfen um das möglichst realistisch zu tun? [/INST] Um den Flug von Papierfliegern in einem Spiel realistisch zu simulieren, gibt es einige wichtige Formeln, die Sie berücksichtigen sollten:\\n\\nLift-Kraft: Der Lift, der einen Papierflieger in der Luft hält, wird durch die Bewegung der Luft über die Oberfläche des Flügels erzeugt. Die Formel zur Berechnung der Lift-Kraft lautet:\\n\\nL = 0.5 * rho * v^2 * A * Cl\\n\\nL = Lift-Kraft\\nrho = Luftdichte\\nv = Geschwindigkeit des Flugzeugs\\nA = Flügeloberfläche\\nCl = Auftriebsbeiwert des Flügels\\n\\nLuftwiderstand: Der Luftwiderstand ist die Kraft, die gegen den Flug eines Papierfliegers wirkt und durch die Bewegung der Luft entsteht, wenn sie um das Flugzeug herumströmt. Die Formel zur Berechnung des Luftwiderstands lautet:\\n\\nD = 0.5 * rho * v^2 * A * Cd\\n\\nD = Luftwiderstand\\nrho = Luftdichte\\nv = Geschwindigkeit des Flugzeugs\\nA = Querschnittsfläche des Flugzeugs\\nCd = Luftwiderstandsbeiwert des Flugzeugs\\n\\nGravitationskraft: Die Gravitationskraft ist die Kraft, die das Flugzeug zur Erde zieht. Die Formel zur Berechnung der Gravitationskraft lautet:\\n\\nFg = m * g\\n\\nFg = Gravitationskraft\\nm = Masse des Flugzeugs\\ng = Beschleunigung durch die Schwerkraft (9,81 m/s^2)\\n\\nGier, Nick- und Rollwinkel: Um die Flugbewegungen eines Papierfliegers realistisch zu simulieren, müssen Sie auch die Gier-, Nick- und Rollwinkel berücksichtigen. Diese Winkel bestimmen die Ausrichtung des Flugzeugs in der Luft und beeinflussen seine Flugbahn.\\n\\nGierwinkel: Der Gierwinkel beschreibt die Drehung des Flugzeugs um die senkrechte Achse. Es wird normalerweise durch den Ruderausschlag des Seitenruders gesteuert.\\nNickwinkel: Der Nickwinkel beschreibt die Drehung des Flugzeugs um die Querachse. Es wird normalerweise durch den Ruderausschlag des Höhenruders gesteuert.\\nRollwinkel: Der Rollwinkel beschreibt die Drehung des Flugzeugs um die Längsachse. Es wird normalerweise durch den Ruderausschlag des Querruders gesteuert.\\nMit diesen Formeln und den entsprechenden Eingabeparametern (z.B. Flugzeuggröße, Fluggeschwindigkeit, Luftdichte) können Sie den Flug von Papierfliegern in Ihrem Spiel realistisch simulieren. Beachten Sie jedoch, dass auch andere Faktoren wie z.B. Wind und Turbulenzen einen Einfluss auf den Flug haben können, und dass die Simulation möglicherweise aufwendig sein kann. </s><s>[INST] Könntest du mir die Formel bitte als Methoden und C# programmieren, damit ich sie in der Game engine Unity verwenden kann? [/INST] Hier ist eine C#-Klasse, die die Formeln für Lift und Luftwiderstand implementiert. Gravitation ließe sich zwar ebenfalls leicht implementieren, ist in Unity aber bereits eingebaut.\\n\\n```csharp\\nclass PaperPlanePhysics {\\n\\t// Physikalische Konstanten\\n\\tpublic float rho = 1.2041f; // Luftdichte, kg/m³\\n\\t\\n\\t// Parameter des Papierfliegers\\n\\tpublic float A; // Flügeloberfläche, m²\\n\\tpublic float Cl; // Auftriebsbeiwert\\n\\tpublic float Cd; // Luftwiderstandsbeiwert\\n\\t\\n\\t// Auftriebskraft\\n\\tpublic float ComputeLift(float v) {\\n\\t\\treturn 0.5f * rho * v*v * A * Cl;\\n\\t}\\n\\t\\n\\t// Luftwiderstand\\n\\tpublic float ComputeDrag(float v) {\\n\\t\\treturn 0.5f * rho * v*v * A * Cd;\\n\\t}\\n\\t\\n\\t// Resultierende Kräfte als Vektor berechnen\\n\\tpublic Vector3 ComputeForceVector(Vector3 velocity, Vector3 forward, Vector3 up) {\\n\\t\\t// Windgeschwindigkeit glatt entlang des Flügels\\n\\t\\tfloat forwardVelocity = Vector3.Dot(velocity, forward);\\n\\t\\t\\n\\t\\t// Vereinfachung: In der Realität ist die Abhängigkeit vom Winkel zwischen Flügel und Luftgeschwindigkeit komplexer.\\n\\t\\t// Nehmen wir an dass es nur von der Geschwindigkeit gerade nach vorne abhängt.\\n\\t\\tVector3 lift = up * ComputeLift(forwardVelocity);\\n\\t\\t\\n\\t\\t// Vereinfachung: Luftwiderstand wirkt in allen Richtungen, möglicherweise mit unterschiedlichen Faktoren.\\n\\t\\t// Wir berechnen hier nur den Luftwiderstand nach vorne.\\n\\t\\tVector3 drag = -forwardVelocity * ComputeDrag(forwardVelocity);\\n\\t\\t\\n\\t\\treturn lift + drag;\\n\\t}\\n}\\n```\\n\\nIn einer Game-Engine wie Unity benötigst du in der Regel einen 2D- oder 3D-Vektor für die Kräfte. Die Methode `ComputeForceVector` ist ein Beispiel dafür wie du die Vektoren berechnen kannst. </s><s>[INST] Ich arbeite international mit anderen Entwicklern zusammen und schreibe deshalb ausschließlich englischen Code. Kannst du die Kommentare bitte übersetzen und etwas aussagekräftigere Variablennamen für die Flügeloberfläche, den Auftrieb und den Luftwiderstand verwenden? [/INST] Gerne! Hier ist der englische Code mit besseren Variablennamen:\\n\\n```cs\\nclass PaperPlanePhysics {\\n\\n\\t// physical constants\\n\\tpublic float rho = 1.2041f; // air density, kg/m³\\n\\t\\n\\t// characteristics of the paper plane\\n\\tpublic float wingSurf; // wing surface, m²\\n\\tpublic float liftCoeff; // lift coefficient\\n\\tpublic float dragCoeff; // drag coefficient\\n\\t\\n\\t// calculate lift force based on velocity v\\n\\tpublic float ComputeLift(float v) {\\n\\t\\treturn 0.5f * rho * (v * v) * wingSurf * liftCoeff;\\n\\t}\\n\\t\\n\\t// calculate air drag based on velocity v\\n\\tpublic float ComputeDrag(float v) {\\n\\t\\treturn 0.5f * rho * (v * v) * wingSurf * dragCoeff;\\n\\t}\\n\\t\\n\\t// calculate all resulting forces as a vector\\n\\tpublic Vector3 ComputeForceVector(Vector3 velocity, Vector3 forward, Vector3 up) {\\n\\t\\t// wind speed along the wing\\n\\t\\tfloat forwardVelocity = Vector3.Dot(velocity, forward);\\n\\t\\t\\n\\t\\t// Simplification: In reality, the relationship between wing angle and airspeed is much more complex.\\n\\t\\t// We assume that it depends only on the speed straight ahead.\\n\\t\\tVector3 lift = up * ComputeLift(forwardVelocity);\\n\\t\\t\\n\\t\\t// Simplification: Air resistance works in all directions, possibly with different factors.\\n\\t\\t// We only compute the forward drag here.\\n\\t\\tVector3 drag = -forwardVelocity * ComputeDrag(forwardVelocity);\\n\\t\\t\\n\\t\\treturn lift + drag;\\n\\t}\\n}\\n``` </s>'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilgpt2\"\n",
    "import transformers\n",
    "# in transformers library, there is a base class called AutoModel\n",
    "# AutoModelForCausalLM is a subclass of AutoModel we're using (gpt2 and chatbots are a type of language model called causal language models) \n",
    "# They're called this because they generate one token at a time\n",
    "# If we asked a causal language model to write a poem, it would generate this response one token at a time, until the final response:\n",
    "# This\n",
    "# This is\n",
    "# This is a\n",
    "# This is a funny\n",
    "# This is a funny poem\n",
    "# The tokens are predicted in a cause and effect way - the tokens in the past will influence the next word\n",
    "from transformers import AutoModelForCausalLM       # used to automatically get the model based on the model_name\n",
    "from transformers import AutoTokenizer              # automatically retrieve the right tokenizer object based on the model being used\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map = \"auto\")   # device_map tells the library to automatically move things to the GPU or CPU when necessary\n",
    "model.config.use_cache = True   # we don't need to recompute stuff for previous tokens\n",
    "# This\n",
    "# This is\n",
    "# This is a         -> the only new token is \"a\", cache previous results / hidden states associated with the prior tokens\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)   # tells library to trust code and pretrained model downloaded from internet\n",
    "# during training, we feed in batches of examples at a time, and all the examples may not be the same length \n",
    "# so to make them the same length we use padding, with the padding token being the end of sentence token\n",
    "# as soon as a sentence is over (no more tokens left), padding by repeating end of sentence conveys the same thing - that the sentence is over\n",
    "# we need to clarify we will do our padding on the right so it goes at the end of the sequence, left pad would add end of sentence tokens to the start\n",
    "# also set the integer associated with the padding token to be the same integer associated with the end of sentence token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "generation_configuration = model.generation_config\n",
    "generation_configuration.pad_token_id = tokenizer.eos_token_id\n",
    "generation_configuration.eos_token_id = tokenizer.eos_token_id\n",
    "# max amount of tokens to be generated by model, it can't just go on forever\n",
    "generation_configuration.max_new_tokens = 1024  # gpt-2 has a context length of 1024 (it can only factor in past 1024 tokens for the response)\n",
    "\n",
    "generation_configuration.do_sample = True\n",
    "# variables below relate to how we sample from the model, this affects the quality and how diverse the model's outputs are (super bland vs fun and varied)\n",
    "# divides every raw number in the list (before they get converted to probabilities with softmax) by this constant - affects how diverse / crazy model responses are\n",
    "# low temperature below 1 sharpens distribution and makes higher probs even more likely to be chosen - less diverse\n",
    "# high temperature above 1 flattens distribution and gives increased chance for lower probability tokens to be chosen - more diverse\n",
    "generation_configuration.temperature = 0.7\n",
    "# top-p means we only consider highest probability tokens in the distribution: aggregate their sum and stop considering the rest of the tokens that makes the sum exceed p\n",
    "# discard less likely tokens and don't consider them in sampling process\n",
    "generation_configuration.top_p = 0.9\n",
    "# only consider the k highest probability tokens during the sampling process (the top k tokens are then renormalized to sum to 1)\n",
    "generation_configuration.top_k = 20\n",
    "\n",
    "# model will output a probability distribution of which token should come next: \n",
    "# [0.2, 0.23, 0.69, 0.1, ..., 0.01, 0.14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training + Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)    # omit this if running in colab or with gpu\n",
    "def generate(prompt):\n",
    "    # convert string into integers that the model can understand\n",
    "    # add_special_tokens automatically takes care of BOS, padding, and EOS tokens\n",
    "    # return_tensors = \"pt\" so model knows to use pytorch compatible format and not something like tensorflow\n",
    "    # .to(device) means move to gpu if available\n",
    "    encoded = tokenizer.encode(prompt, add_special_tokens = True, return_tensors = \"pt\").to(device)\n",
    "    # call model's generate passing in the encoded result we got as the token ids / integers\n",
    "    # do_sample as false just picks token with highest probability, we set it as true to actually choose / sample from all probabilities\n",
    "    # repetition_penalty makes the model stop outputting the same thing over and over again\n",
    "    out = model.generate(input_ids = encoded, repetition_penalty = 2.0, do_sample = True)\n",
    "    # out will also be a bunch of integers representing tokens, need to convert it back to the actual strings they represent\n",
    "    # convert output to list instead of keeping it as a tensor, and index first element because we will only ever consider one response to prompt\n",
    "    # could've set more independent responses to be generated based on prompt in generation config\n",
    "    # clean_up_tokenization_spaces cleans up clunky spaces from tokenizer when it converts spaces / BOS / EOS tokens to integers\n",
    "    string_decoded = tokenizer.decode(out[0].tolist(), clean_up_tokenization_spaces = True)\n",
    "    print(string_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the name of the first person to land on the moon was not used.”\n",
      "The term “nostalgia is a combination thereof, in which people are asked for things they don't have time or energy that would otherwise be impossible and therefore cannot do anything wrong if there were no other options available:\n",
      "\n",
      "[1] This means it can also mean something different than being unable (and/or unwilling)to use an old-fashioned way when you want them! [2][3](https://www4u9yfjr6gw8d0). If someone has some form otomaniacal pain caused by these painful experiences then this should probably suffice as long before using any type therapy whatsoever!!<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "generate('the name of the first person to land on the moon was')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is not a good thing.\n",
      "The fact that the only way to do this without making them into something of an art form for anyone else, as long I can see what works better than me and my friends are doing in it now would be very much more effective if we could create things like these with minimal effort from within our own minds - so there was no reason why people couldn't have done anything differently because they didn‍t want their creations being created entirely on screen or at all... But then again: you know how many times does someone say “I am just going through some crazy shit! That person should definitely go out here (if he wants) when trying new stuff!\" And yes, those who did try make fun off of everything but were simply too lazy by default; everyone has been able to use any sorta work ethic while still having such high expectations which makes getting around pretty boring isn´ts quite difficult....So let's start over…<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "generate('this is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are you going to do this?\n",
      "I hope so.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "generate('how are you')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not answering question (how are you), is just trying to complete the rest of the sentence and generate more text. We want to fine-tune the model to function more like a helpful chatbot assistant that will answer these types of questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "from trl import SFTConfig, SFTTrainer  # use SFT instead of vanilla trainer as it's highly optimized for quickly training / fine-tuning transformers\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    gradient_accumulation_steps=1,      # don't need to use the gradient hack described below\n",
    "    num_train_epochs=1,                 # fine-tuning doesn't require a crazy amount of training, just need 1 epoch (num of passes made over entire training dataset)\n",
    "    learning_rate=2e-4,                 # learning rate for gradient descent\n",
    "    # fp16=True,                        # include if running on colab or have access to gpu\n",
    "    output_dir=\"logs\",                  # put any logging or predictions it makes in an output directory called logs\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    # During training we should process examples with the same length together, rather than having a bunch of variable length examples and having to do a lot of padding / truncation\n",
    "    group_by_length=True,\n",
    "    max_length=512,                     # maximum training length sequence - the batches of text from the dataset - we use is 512\n",
    "    dataset_text_field=\"text\"\n",
    ")\n",
    "\n",
    "# batch size is how many examples in parallel we train on at every iteration\n",
    "# sometimes our GPUs have limited memory, which makes it so the batch size cannot be very large (the GPU can't load / process in so many examples in parallel at the same time)\n",
    "\n",
    "# a hack to get around this: at every iteration of training don't update the weights\n",
    "# instead: do an iteration of training (with whatever batch size the GPU can handle), calculate gradients, but don't take a step / update the weights\n",
    "# if gradient accumulation steps was 2, then do another iteration, calculate gradients, ADD them to the previous iteration's gradients\n",
    "# AND THEN update the weights\n",
    "# even though we couldn't process 2 * batch_size example all in one iteration in parallel on the same GPU, we simulated it by splitting it up\n",
    "# we simulate the effect of the model updating its weights only after 2 * batch_size examples\n",
    "\n",
    "# We know learning rate is critical to gradient descent (too small makes training take forever, too high makes weights change by a crazy amount every iteration - model doesn't learn well)\n",
    "# Dynamic learning rate is best (dynamic alpha)\n",
    "# Use a warmup by starting the learning rate really small and increasing it, then schedule the learning rate based on the cosine function\n",
    "# Cosine goes up and down, this function bases how the training algorithm will vary and adjust the learning rate\n",
    "# When we start out, we don't want a super high learning rate (we're still seeing how is the model adjusting and how is the error changing in response to the learning rate)\n",
    "# In the middle of training we want a higher learning rate as that's where the bulk of the work is going on\n",
    "# Towards the end of training, we want small learning rate again, because we're getting really close to where we want to be - don't want more drastic changes to model\n",
    "\n",
    "# define our trainer object, max_seq_length=512 \n",
    "trainer = SFTTrainer(model=model, train_dataset=training_dataset, processing_class=tokenizer, args=training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n",
      "/opt/miniconda3/envs/ml/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 03:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.322400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.354200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.690400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.403300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.334400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.485700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.372600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.222400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.360300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.234900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.328600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.459900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=3.388261947631836, metrics={'train_runtime': 239.5046, 'train_samples_per_second': 4.175, 'train_steps_per_second': 0.522, 'total_flos': 94701699661824.0, 'train_loss': 3.388261947631836, 'entropy': 3.484979820251465, 'num_tokens': 363618.0, 'mean_token_accuracy': 0.37315970063209536, 'epoch': 1.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how are you really just simply to and.\n",
      "\" Please please Thank thank thanks Thanks -, all All of on P P Par Par par par get free Free FreeFreefreefreeFREE FREE F fFffffffffffffffffffffffFFFFFF Fantasy fantasy story adventure AdventureAdventureAdventureAdventureAdventureAdventure Adventures adventures explore exploration exploring exploration Exploration \" for ())))))))  3 4<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "model.to(device)    # omit this if running in colab or with gpu\n",
    "generate('how are you')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore the [/INST], is just the model saying the tokens from that point onwards are the response to the question. We could do preprocessing in the generate function to trim it out. The model actually gives coherent response to the question now, instead of just completing the sentence. The results are decent, but not fantastic. After some number of characters, the model seems to just start rambling about random stuff that doesn't answer our question anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's strive for better results (what can we do?)\n",
    "\n",
    "1. Use a larger model (Billion scale models, like Llama?)\n",
    "2. Use more interesting datasets (medical datasets, talk like Trump, be good at LeetCode)\n",
    "3. Use state of the art tricks (LoRA and qLoRA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
